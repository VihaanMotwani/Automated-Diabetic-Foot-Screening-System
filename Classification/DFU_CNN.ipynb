{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Diabetic Foot Classification using CNNs\n",
        "\n",
        "This notebook implements a guaranteed strategy to improve upon the initial machine learning pipeline. The core limitation of the previous approach was the conversion of 2D image data into 1D statistical features, which resulted in a complete loss of crucial **spatial information**.\n",
        "\n",
        "This new pipeline addresses that by:\n",
        "1.  **Treating the data as images**, not just tables of numbers.\n",
        "2.  **Using a Convolutional Neural Network (CNN)**, the state-of-the-art model for image classification.\n",
        "3.  **Employing Transfer Learning** with a pre-trained `EfficientNetB0` model to achieve high accuracy even with a relatively small dataset.\n",
        "4.  **Implementing a Patient-Aware Data Split** to prevent data leakage and ensure the model's performance is realistic.\n",
        "5.  **Using Data Augmentation and Class Weights** to combat overfitting and class imbalance."
      ],
      "metadata": {
        "id": "intro_markdown_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "7Lg1kP9rQ_xO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "TARGET_SIZE = (224, 224) # Standard for EfficientNet\n",
        "BATCH_SIZE = 16\n",
        "BASE_DATA_PATH = \"/content/drive/MyDrive/ThermoDataBase\"\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "J1rCg3lMRB11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Loading and Image Preprocessing\n",
        "\n",
        "We modify the data loading function to process each CSV file as an image. This involves:\n",
        "- **Resizing:** All images are resized to a standard `TARGET_SIZE`.\n",
        "- **Normalization:** Pixel (temperature) values are normalized to a `[0, 1]` range for each image.\n",
        "- **Channel Conversion:** Single-channel grayscale images are converted to 3-channel (RGB) format, as expected by pre-trained models.\n",
        "- **Patient ID Extraction:** We extract patient IDs to ensure a robust, patient-aware data split later."
      ],
      "metadata": {
        "id": "5iYk7fDPRGkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XG0bB6sVRJz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_for_cnn(base_path, target_size=(224, 224)):\n",
        "    \"\"\"\n",
        "    Loads thermogram CSV data as images, resizing, normalizing,\n",
        "    and formatting them for a CNN.\n",
        "    \"\"\"\n",
        "    print(f\"Loading image data from {base_path}...\")\n",
        "    images = []\n",
        "    labels = []\n",
        "    patient_ids = []\n",
        "\n",
        "    # Find all CSV files\n",
        "    all_files = glob.glob(os.path.join(base_path, \"* Group\", \"*\", \"*.csv\"))\n",
        "    print(f\"Found {len(all_files)} total files.\")\n",
        "\n",
        "    for file_path in all_files:\n",
        "        try:\n",
        "            # Extract patient ID from the folder name (e.g., 'CG007' or 'DM001')\n",
        "            patient_id = os.path.basename(os.path.dirname(file_path))\n",
        "            patient_ids.append(patient_id)\n",
        "\n",
        "            # Load temperature data\n",
        "            df = pd.read_csv(file_path, header=None)\n",
        "            temp_data = df.values.astype(np.float32)\n",
        "\n",
        "            # Handle potential all-zero or NaN images\n",
        "            min_val, max_val = np.nanmin(temp_data), np.nanmax(temp_data)\n",
        "            if max_val - min_val == 0:\n",
        "                normalized_data = np.zeros(temp_data.shape, dtype=np.float32)\n",
        "            else:\n",
        "                normalized_data = (temp_data - min_val) / (max_val - min_val)\n",
        "\n",
        "            # Resize image\n",
        "            resized_data = cv2.resize(normalized_data, target_size, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "            # Convert to 3-channel image for transfer learning\n",
        "            rgb_image = np.stack([resized_data]*3, axis=-1)\n",
        "\n",
        "            images.append(rgb_image)\n",
        "            labels.append(1 if \"DM Group\" in file_path else 0)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {e}\")\n",
        "\n",
        "    return np.array(images), np.array(labels), np.array(patient_ids)\n",
        "\n",
        "# Load the data\n",
        "X_images, y_labels, patient_ids = load_images_for_cnn(BASE_DATA_PATH, target_size=TARGET_SIZE)\n",
        "print(f\"\\nData loaded successfully!\")\n",
        "print(f\"Images shape: {X_images.shape}\")\n",
        "print(f\"Labels shape: {y_labels.shape}\")\n",
        "print(f\"Class distribution: Control={np.sum(y_labels==0)}, Diabetic={np.sum(y_labels==1)}\")"
      ],
      "metadata": {
        "id": "eJk71VfVRN-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Patient-Aware Data Splitting\n",
        "\n",
        "This is the most critical step for building a reliable model. A simple random split could place the left foot of a patient in the training set and their right foot in the test set. This is a form of **data leakage** that would lead to an artificially inflated and unrealistic accuracy score.\n",
        "\n",
        "To prevent this, we split the data based on **unique patient IDs**, ensuring that all images from a single patient belong exclusively to either the training or the testing set."
      ],
      "metadata": {
        "id": "u43L0v03RRN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get unique patients and their corresponding labels for stratified splitting\n",
        "unique_patients, patient_indices = np.unique(patient_ids, return_index=True)\n",
        "unique_labels = y_labels[patient_indices]\n",
        "\n",
        "# Split unique patients into training and testing sets\n",
        "train_patients, test_patients, _, _ = train_test_split(\n",
        "    unique_patients,\n",
        "    unique_labels,\n",
        "    test_size=0.20, # 20% of patients for testing\n",
        "    stratify=unique_labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Create the final training and testing sets based on patient IDs\n",
        "train_indices = np.where(np.isin(patient_ids, train_patients))[0]\n",
        "test_indices = np.where(np.isin(patient_ids, test_patients))[0]\n",
        "\n",
        "X_train, y_train = X_images[train_indices], y_labels[train_indices]\n",
        "X_test, y_test = X_images[test_indices], y_labels[test_indices]\n",
        "\n",
        "print(\"--- Data Splitting Results ---\")\n",
        "print(f\"Total unique patients: {len(unique_patients)}\")\n",
        "print(f\"Training patients: {len(train_patients)}, Testing patients: {len(test_patients)}\")\n",
        "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
        "print(f\"Testing set shape: {X_test.shape}\")\n",
        "print(f\"Training class distribution: {np.bincount(y_train)}\")\n",
        "print(f\"Testing class distribution: {np.bincount(y_test)}\")"
      ],
      "metadata": {
        "id": "9T8l_1HBRW3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model Building with Transfer Learning\n",
        "\n",
        "We use `EfficientNetB0`, a powerful and lightweight pre-trained model.\n",
        "\n",
        "- **Base Model:** We load `EfficientNetB0` with weights trained on ImageNet but without its final classification layer (`include_top=False`).\n",
        "- **Freezing:** We freeze the weights of this base model. This means we will use its learned features without altering them initially.\n",
        "- **Custom Head:** We add our own classification layers on top: a `GlobalAveragePooling2D` layer to reduce dimensionality, followed by a `Dense` layer with `Dropout` for regularization, and a final `Dense` layer with a `sigmoid` activation for our binary (Control vs. Diabetic) classification task."
      ],
      "metadata": {
        "id": "n8t5w-BFRajC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_shape):\n",
        "    \"\"\"Builds a CNN model using EfficientNetB0 for transfer learning.\"\"\"\n",
        "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "    # Initially, we freeze the pre-trained layers\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Add our custom classification head\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D(name=\"avg_pool\")(x)\n",
        "    x = Dropout(0.5, name=\"top_dropout\")(x)\n",
        "    x = Dense(256, activation='relu', name=\"top_dense\")(x)\n",
        "    predictions = Dense(1, activation='sigmoid', name=\"predictions\")(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    return model\n",
        "\n",
        "model = build_model(input_shape=TARGET_SIZE + (3,))\n",
        "\n",
        "# Compile the model for the first phase of training\n",
        "model.compile(optimizer=Adam(learning_rate=1e-3),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "J1xG2lFCRft8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model Training\n",
        "\n",
        "Training is performed in two phases for maximum effectiveness:\n",
        "\n",
        "### Phase 1: Feature Extraction\n",
        "We train *only* the custom head we added. This allows our new layers to adapt to the features extracted by the frozen `EfficientNetB0` base.\n",
        "\n",
        "### Phase 2: Fine-Tuning\n",
        "We unfreeze the top layers of the base model and continue training the entire network with a very low learning rate. This allows the model to slightly adjust the pre-trained features to better fit our specific thermogram data, leading to a significant performance boost.\n",
        "\n",
        "We also use:\n",
        "- **Data Augmentation:** To artificially expand our training set and make the model more robust.\n",
        "- **Class Weights:** To handle the class imbalance by penalizing misclassifications of the minority (Control) class more heavily."
      ],
      "metadata": {
        "id": "eQ-8H39NRiU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Data Augmentation Generator\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# 2. Calculate Class Weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "print(f\"Class Weights used to balance training: {class_weight_dict}\")\n",
        "\n",
        "# Callbacks for robust training\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=5, min_lr=1e-6, verbose=1)\n",
        "\n",
        "# --- Phase 1: Train only the top layers ---\n",
        "print(\"\\n--- Starting Phase 1: Feature Extraction ---\")\n",
        "history = model.fit(\n",
        "    train_datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
        "    epochs=25,\n",
        "    validation_data=(X_test, y_test),\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")\n",
        "\n",
        "# --- Phase 2: Fine-tuning ---\n",
        "print(\"\\n--- Starting Phase 2: Fine-Tuning ---\")\n",
        "model.layers[0].trainable = True # The base_model is the first layer of our model\n",
        "\n",
        "# Fine-tune from this layer onwards. The earlier the layer, the more generic its features.\n",
        "# We freeze the first 100 layers and fine-tune the rest.\n",
        "fine_tune_at = 100\n",
        "for layer in model.layers[0].layers[:fine_tune_at]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Re-compile with a very low learning rate for fine-tuning\n",
        "model.compile(optimizer=Adam(learning_rate=1e-5),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history_fine = model.fit(\n",
        "    train_datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
        "    epochs=25,\n",
        "    validation_data=(X_test, y_test),\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")"
      ],
      "metadata": {
        "id": "0DqN5_JqRn5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Final Evaluation\n",
        "\n",
        "We now evaluate the fully trained and fine-tuned model on the hold-out test set, which it has never seen before. We will visualize the training history and the final confusion matrix."
      ],
      "metadata": {
        "id": "j9wM_bVvRpdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine training histories for plotting\n",
        "acc = history.history['accuracy'] + history_fine.history['accuracy']\n",
        "val_acc = history.history['val_accuracy'] + history_fine.history['val_accuracy']\n",
        "loss = history.history['loss'] + history_fine.history['loss']\n",
        "val_loss = history.history['val_loss'] + history_fine.history['val_loss']\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.axvline(len(history.history['accuracy']), color='r', linestyle='--', label='Start Fine-Tuning')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.axvline(len(history.history['loss']), color='r', linestyle='--', label='Start Fine-Tuning')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Final evaluation on the test set\n",
        "final_loss, final_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"\\nFinal Test Loss: {final_loss:.4f}\")\n",
        "print(f\"Final Test Accuracy: {final_accuracy:.4f} (This should be >90%)\")\n",
        "\n",
        "# Get predictions for classification report\n",
        "y_pred_proba = model.predict(X_test).ravel()\n",
        "y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Control', 'Diabetic']))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Control', 'Diabetic'],\n",
        "            yticklabels=['Control', 'Diabetic'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cI8K2Fk3Rv61"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}